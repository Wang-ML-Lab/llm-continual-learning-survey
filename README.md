# A Survey on Continual Learning in the Era of Large Language Models
An ongoing project surveying existing studies on Continual Learning x Large Language Models.

## Relevant Survey Papers
- A Comprehensive Survey of Continual Learning: Theory, Method and Application (TPAMI 2024) [[paper](https://arxiv.org/abs/2302.00487)]
- Continual Learning for Large Language Models: A Survey [[paper](https://arxiv.org/abs/2402.01364)]
- Continual Lifelong Learning in Natural Language Processing: A Survey (COLING 2020) [[paper](https://arxiv.org/abs/2012.09823)]
- Continual Learning of Natural Language Processing Tasks: A Survey [[paper](https://arxiv.org/abs/2211.12701)]
- A Survey on Knowledge Distillation of Large Language Models [[paper](https://arxiv.org/abs/2402.13116)]


## Continual Pre-Training of LLMs (CPT)


## Domain-Adaptive Pre-Training of LLMs (DAP)


## Continual Fine-Tuning of LLMs (CFT)

### General Continual Fine-Tuning

### Continual Instruction Tuning (CIT)

### Continual Model Refinement (CMR)

### Continual Model Alignment (CMA)

- Alpaca: A Strong, Replicable Instruction-Following Model [[project](https://crfm.stanford.edu/2023/03/13/alpaca.html)][[code](https://github.com/tatsu-lab/stanford_alpaca)]

- Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems [[paper](https://arxiv.org/pdf/2108.12589.pdf)]

- Training language models to follow instructions with human feedback [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)]

- Direct preference optimization: Your language model is secretly a reward model [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf)]

- Copf: Continual learning human preference through optimal policy fitting [[paper](https://arxiv.org/pdf/2310.15694)]

- CPPO: Continual Learning for Reinforcement Learning with Human Feedback] [[paper](https://openreview.net/pdf?id=86zAUE80pP)]

- A Moral Imperative: The Need for Continual Superalignment of Large Language Models [[paper](https://arxiv.org/pdf/2403.14683)]

- Mitigating the Alignment Tax of RLHF [[paper](https://arxiv.org/abs/2309.06256)]

### Continual Multimodal LLMs (CMLLMs)
